{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"pytorch_env","language":"python","name":"pytorch_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"day14.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"J4ZNXhW1yi6x"},"source":["### 作業目的: 更加了解word2vec高速化\n","本次作業主要是幫同學更熟悉與了解透過各項技巧來加速word2vec的原理，同學可以參考章節講義來回答下列問題。"]},{"cell_type":"markdown","metadata":{"id":"TsxMYpkLyi65"},"source":["### Q1 - 請問word2vec原本的設計有何問題以及可以怎麼對相對應的問題做改善?\n","\n","\n","Answer:\n","\n","## word2vec 在文本資料庫過大時，每個詞的 one-hot encoding 的維度會過大，造成訓練時參數量也跟著過大，並且模型在最終預測分類的 softmax 的維度亦會過大。##\n","\n","## 本章節提到透過 Hierachical softmax 與 Nagative Sampling 來處理上述問題 ##"]},{"cell_type":"markdown","metadata":{"id":"GluCcZIGyi66"},"source":["### Q2 - 請問在Negative Sampling中的次方係數，會如何影響字詞的抽取?\n","Hint: 如何影響高頻詞與低頻詞的抽取機率\n","\n","Answer:\n","## 相對於沒將詞頻取 0.75 次方的計算版本，有取 0.75 次方的計算方式會使出現頻率高的詞被抽中的機率下降一些，相反的，出現頻率較低的詞被抽到的機率會些微上升##"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIgqQYBOyi67","executionInfo":{"status":"ok","timestamp":1619777773900,"user_tz":-480,"elapsed":3199,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"07284fb7-519e-49f9-b000-81f4e3b99cb7"},"source":["print(7, 7**0.75)\n","print(3, 3**0.75)\n","print(7**0.75/(7**0.75+3**0.75), 3**0.75/(7**0.75+3**0.75))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["7 4.303517070658851\n","3 2.2795070569547775\n","0.6537294998824336 0.34627050011756644\n"],"name":"stdout"}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"cupoy_env","language":"python","name":"cupoy_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"day11.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"e3kF8sgRtqbB"},"source":["### 作業目的: 了解為何需要推論方法的詞向量及其優缺點\n","\n","本次作業主要為思考題，請學員根據題目思考適合的回答"]},{"cell_type":"markdown","metadata":{"id":"mquQ7rrStqbI"},"source":["### Q1:\n","請學員思考為何不直接使用one-hot encoding的方式建立詞向量，而要有其他計數方法與推論方法的詞向量產生方式？"]},{"cell_type":"markdown","metadata":{"id":"j4fPPPpLtqbJ"},"source":["### Answer:\n","### 1.當文本中的詞較大，字典的 size 較大時會造成 one-hot encoding 的維度太大，造成較大的計算量\b###\n","### 2.當詞以 one-hot encoding 來表示時，向量間皆為正交，因此相近的詞語並沒有較為相近的表示。"]},{"cell_type":"markdown","metadata":{"id":"yzcFvd52tqbJ"},"source":["### Q2:\n","相較於計數手法的詞向量(ex: one-hot, 共現矩陣, PPMI)，word2vec的方法有何優點？"]},{"cell_type":"markdown","metadata":{"id":"YhFQ_hAJtqbK"},"source":["### Answer:\n","### 相對於共現矩陣，一次性使用 SVD 來獲得字詞的稠密矩陣表示，因此當需要處理文本的詞彙量很龐大時，計算成本會太過昂貴，而且當有新詞彙加入時，計數手法必須從頭開始計算生成字詞的稠密矩陣。###\n","### word2vec 則是使用類神經網路以小批次 (mini-batch) 進行學習生成詞向量的模型。隱藏層為簡單的全連接層，可透過 GPU 加速運算。"]}]}
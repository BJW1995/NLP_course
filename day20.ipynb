{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"day20.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"q0y1IFdUh5zs"},"source":["#  作業: 使用 LSTM 做文本情感分析"]},{"cell_type":"markdown","metadata":{"id":"5d9-7dY6rjbe"},"source":["##[作業目標]\n","\n","*   使用 Pytorch 提供的 LSTM 方法來做情感(情緒)的分析\n","*   期望達到不錯的準確度，84% 以上"]},{"cell_type":"markdown","metadata":{"id":"0d6gKxOKETRU"},"source":["##[作業重點]\n","\n","*   學會使用 torchtext dataset 來使用 IMDB 資料集\n","*   搭建 LSTM 網路\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WtKPzAnqh5zs"},"source":["## 準備資料\n","\n","torchtext 包含以下 components：\n","\n","Field : 主要包含以下數據預處理的配置信息：指定分詞方法、是否轉成小寫、起始符號、以及字典等等。\n","\n","Dataset : 用於下載數據，也提供 splits 方法可以同時下載訓練資料、驗證資料和測試資料。\n","\n","Iterator : 數據讀取的迭代器，可以支持 batch\n","\n","我們定義 SEED、TEXT 和 LABEL 三個變數來隨機把資料集分割成 train/valid/test 三個資料集。\n","\n"]},{"cell_type":"code","metadata":{"id":"BpIzg9Ehh5zs","executionInfo":{"status":"ok","timestamp":1620526438833,"user_tz":-480,"elapsed":9129,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["import torch\n","import torch.nn as nn\n","from torchtext.legacy import data, datasets\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5im_3jPuh5zs"},"source":["## 下載並讀取資料\n","\n","torchtext 的 datasets 集合裡面就有 IMDB 資料，直接就可以讀取訓練以及測試資料了。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQBm1Pihh5zs","executionInfo":{"status":"ok","timestamp":1620527275609,"user_tz":-480,"elapsed":829331,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"4edcd72d-053a-478e-a15d-69588bb22ed4"},"source":["train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 23.2MB/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lg3dDAnvh5zs"},"source":["\n","## 從訓練資料裡面切割驗證資料\n","\n","從訓練資料裡面抓取一些資料當作 validation set"]},{"cell_type":"code","metadata":{"id":"LJ3m5a4jh5zs","executionInfo":{"status":"ok","timestamp":1620527319383,"user_tz":-480,"elapsed":885,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["import random\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rt_o8fRh5zs"},"source":["# 建立字典\n","\n","接下來是使用預訓練的 word embeddings。只要呼叫 TorchText 的 build_vocab 就可以把所有的文字向量化, 我們使用的是 \"glove.6B.100d\" 的向量，glove 是一個用來計算詞向量的演算法。6B 是指這些詞向量是用了60億個tokens訓練出來的，而 100d 是指每一個向量的維度是 100。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y855ENTLh5zs","executionInfo":{"status":"ok","timestamp":1620527535450,"user_tz":-480,"elapsed":210631,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"1bb78dd0-3b17-4041-98a6-575a0ecbc9f2"},"source":["MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\",\n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)"],"execution_count":4,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n","100%|█████████▉| 399917/400000 [00:14<00:00, 26307.44it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"KTJfJYW_h5zs"},"source":["#建立Iterator\n","\n","Iterator 是 torchtext 到模型的輸出，提供了對數據的打亂、排序等等處理方法。可以動態修改 batch size這裡使用 splits method 來同時輸出訓練集、驗證集以及測試集。\n","\n","如果有 GPU 的話則使用 cuda 來做運算。\n","\n","`sort_within_batch = True` 是表示在每一個 batch 裡面的 tensors 是依照長度排序的。"]},{"cell_type":"code","metadata":{"id":"KqCebFvAh5zs","executionInfo":{"status":"ok","timestamp":1620529075342,"user_tz":-480,"elapsed":814,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGpIKauzh5zs"},"source":["# 建立 LSTM 模型\n","我們將使用 pytorch 內建的 RNN 架構是 LSTM (Long Short-Term Memory)模型。它的公式如下：\n","\n","$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$\n","\n","\n","步驟解釋：\n","\n","1. 在模型裡，每個詞會先通過 embedding layer 的到特徵向量\n","2. 然後我們使用 LSTM 對特徵序列進一步編碼得到序列信息。\n","3. 將編碼後的序列信息通過全連接層(Fully connectivity layer)得到輸出。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"hjCYAR_Eh5zs","executionInfo":{"status":"ok","timestamp":1620529219394,"user_tz":-480,"elapsed":734,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["\n","class LSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        \n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        #pack sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\n","        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4s7IySNeh5zs"},"source":["\n","# LSTM 模型參數說明\n","\n","1. vocab_size: 輸入層的維度(input dim)\n","2. embedding_dim: 詞向量的維度, 我們使用的是 glove.6B.100d, 所以這裡 embedding_dim 是 100\n","3. hidden_dim: the size of the hidden states\n","4. output_dim: 輸出層的維度\n","5. n_layers: 有幾層全連結層\n","6. bidirectional: 是否使用雙向 RNN\n","7. dropout： dropout 的比例\n","8. pad_idx: token <pad> 的 index\n"]},{"cell_type":"code","metadata":{"id":"0nRixhDvh5zs","executionInfo":{"status":"ok","timestamp":1620529266731,"user_tz":-480,"elapsed":896,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = False\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = LSTM(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT,\n","            PAD_IDX)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpYQJmGih5zs"},"source":["印出我們模型的參數量"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"clRTL5Hih5zs","executionInfo":{"status":"ok","timestamp":1620529272048,"user_tz":-480,"elapsed":685,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"d58fa8cf-e6a3-44b0-8564-e4db4f3e1564"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The model has 3,393,641 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iBc7QkGjh5zt"},"source":["\n","檢查 embedding 的字典大小以及 embedding 的維度\n","\n","_**[vocab size, embedding dim]**_ \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1k7sY2aCh5zt","executionInfo":{"status":"ok","timestamp":1620529278397,"user_tz":-480,"elapsed":673,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"622dd51e-5318-415c-a585-84b2bcf891fe"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","\n","print(pretrained_embeddings.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["torch.Size([25002, 100])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j2BPDdj-h5zt"},"source":["用 pre-trained embeddings 來當作 `embedding` 層的初始化參數\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUZmehuKh5zt","executionInfo":{"status":"ok","timestamp":1620529287417,"user_tz":-480,"elapsed":850,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"492408e2-71d5-43f4-c3e0-f1b3251c4da6"},"source":["model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n","        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-0.0540,  0.6001, -0.0981,  ..., -0.2348,  0.0206,  0.4487],\n","        [ 0.4160,  0.1364,  0.4407,  ...,  0.0690, -0.2379, -0.0471],\n","        [-0.5045,  1.0418,  0.3279,  ...,  0.6332, -0.4239,  0.6381]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"yAO6zCL9h5zt"},"source":["因為`<unk>` and `<pad>`是沒有在 pre-trained 的詞裡面，所以要把 `<unk>` 和 `<pad>`的 初始 embedding 權重都變成 0"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dy8PyFGLh5zu","executionInfo":{"status":"ok","timestamp":1620529295324,"user_tz":-480,"elapsed":817,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"36c4b941-4128-451e-b4e8-2011323ff141"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-0.0540,  0.6001, -0.0981,  ..., -0.2348,  0.0206,  0.4487],\n","        [ 0.4160,  0.1364,  0.4407,  ...,  0.0690, -0.2379, -0.0471],\n","        [-0.5045,  1.0418,  0.3279,  ...,  0.6332, -0.4239,  0.6381]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mbublLfuh5zu"},"source":["We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."]},{"cell_type":"markdown","metadata":{"id":"mQ3KLo-Bh5zu"},"source":["## 訓練模型"]},{"cell_type":"markdown","metadata":{"id":"ZOyVlzOgh5zu"},"source":["使用優化器 Adam"]},{"cell_type":"code","metadata":{"id":"z4pq14aXh5zu","executionInfo":{"status":"ok","timestamp":1620529302469,"user_tz":-480,"elapsed":915,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJ-Obpa0h5zu"},"source":["使用 BCEWithLogitsLoss 當作 Loss Function"]},{"cell_type":"code","metadata":{"id":"mTqpIgdmh5zu","executionInfo":{"status":"ok","timestamp":1620529316704,"user_tz":-480,"elapsed":11474,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aj_qyWU1h5zu"},"source":["實作計算計算準確度的函式\n","\n"]},{"cell_type":"code","metadata":{"id":"bWMRD9Yfh5zu","executionInfo":{"status":"ok","timestamp":1620529379263,"user_tz":-480,"elapsed":804,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QF66HLSsh5zu"},"source":["# 訓練函式"]},{"cell_type":"code","metadata":{"id":"nYmpftoIh5zu","executionInfo":{"status":"ok","timestamp":1620529383665,"user_tz":-480,"elapsed":998,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["\n","\n","def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        text, text_lengths = batch.text\n","        \n","        predictions = model(text, text_lengths).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZmY5b9hh5zu"},"source":["# 測試模型的方法\n"]},{"cell_type":"code","metadata":{"id":"7i7SLjYoh5zu","executionInfo":{"status":"ok","timestamp":1620529387248,"user_tz":-480,"elapsed":854,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text, text_lengths = batch.text\n","            \n","            predictions = model(text, text_lengths).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1Ygz7V-h5zu"},"source":["查看訓練進度以及花費的時間"]},{"cell_type":"code","metadata":{"id":"3iCSdOs8h5zu","executionInfo":{"status":"ok","timestamp":1620529391723,"user_tz":-480,"elapsed":791,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bi8X4DTgh5zu"},"source":["開始訓練"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uav7h10Nh5zu","executionInfo":{"status":"ok","timestamp":1620529483514,"user_tz":-480,"elapsed":89066,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"ae893123-0262-4d9d-c4ef-8a3286254804"},"source":["N_EPOCHS = 5\n","\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.679 | Train Acc: 57.81%\n","\t Val. Loss: 0.658 |  Val. Acc: 64.45%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.633 | Train Acc: 65.02%\n","\t Val. Loss: 0.677 |  Val. Acc: 51.76%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.599 | Train Acc: 68.15%\n","\t Val. Loss: 0.432 |  Val. Acc: 81.27%\n","Epoch: 04 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.473 | Train Acc: 77.37%\n","\t Val. Loss: 0.403 |  Val. Acc: 83.39%\n","Epoch: 05 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.397 | Train Acc: 82.51%\n","\t Val. Loss: 0.336 |  Val. Acc: 85.35%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Xllf06gh5zu"},"source":["測試模型的準確度"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zrPaGTBh5zu","executionInfo":{"status":"ok","timestamp":1620529643264,"user_tz":-480,"elapsed":7786,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"d4752fc7-1ebd-4afa-920b-d76cfca00230"},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Test Loss: 0.351 | Test Acc: 84.53%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lo2tHWvLh5zu"},"source":["## Demo 函式\n","\n","最後我們來建立一個 Demo 的函式讓使用者可以輸入任意的句子來看看模型是否可以正確的做好情緒分類。"]},{"cell_type":"code","metadata":{"id":"N1dQ2jL6h5zu","executionInfo":{"status":"ok","timestamp":1620529659537,"user_tz":-480,"elapsed":2088,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}}},"source":["import spacy\n","nlp = spacy.load('en')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","    prediction = torch.sigmoid(model(tensor, length_tensor))\n","    return prediction.item()"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNTmEPO2h5zu"},"source":["# Demo"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdyUrek4h5zu","executionInfo":{"status":"ok","timestamp":1620529664346,"user_tz":-480,"elapsed":762,"user":{"displayName":"王博正","photoUrl":"","userId":"01330688402941364671"}},"outputId":"b945f2ce-5d1c-4771-f44c-c514b0c5a339"},"source":["sentences = [\"This film is bad\", \"This film is good\"]\n","for sentence in sentences:\n","  if predict_sentiment(model, sentence) > 0.5:\n","    print(\"{} --> Positive\".format(sentence))\n","  else:\n","    print(\"{} --> Negtive\".format(sentence))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["This film is bad --> Negtive\n","This film is good --> Positive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ByB6QdaFh5zu"},"source":["## 參考資料\n","\n","torchtext 入門\n","\n","https://zhuanlan.zhihu.com/p/31139113"]}]}